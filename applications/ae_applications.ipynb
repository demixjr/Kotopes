{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑: D:/Kotopes/Kotopes/data\n",
      " –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: 16130 –∑–æ–±—Ä–∞–∂–µ–Ω—å\n",
      "   –ö–ª–∞—Å–∏: ['train', 'val']\n",
      "üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ: 14500 —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö, 1500 —Ç–µ—Å—Ç–æ–≤–∏—Ö\n",
      " –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: cpu\n",
      "–ü–æ—á–∞—Ç–æ–∫ —à–≤–∏–¥–∫–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...\n",
      "Epoch 1/8: Loss: 0.0341\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import sys\n",
    "\n",
    "FAST_MODE = True\n",
    "SUBSET_SIZE = 14500  # –¢—ñ–ª—å–∫–∏ 500 –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è!\n",
    "TEST_SIZE = 1500    # –¢—ñ–ª—å–∫–∏ 100 –¥–ª—è —Ç–µ—Å—Ç—É\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 8         # –î—É–∂–µ –º–∞–ª–æ –µ–ø–æ—Ö!\n",
    "LATENT_DIM = 16    # –î—É–∂–µ –º–∞–ª–∏–π latent space\n",
    "\n",
    "# --- Cell 1: –ê–†–•–Ü–¢–ï–ö–¢–£–†–ê ---\n",
    "class FastAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=16):\n",
    "        super(FastAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1), # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 16 * 16, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16 * 16 * 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (16, 16, 16)),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# --- Cell 2: –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø (–í–ò–ü–†–ê–í–õ–ï–ù–ê –í–ï–†–°–Ü–Ø) ---\n",
    "def load_tiny_dataset(data_path, img_size=64):\n",
    "    \"\"\"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∏–±—ñ—Ä–∫—É –¥–∞–Ω–∏—Ö\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —ñ—Å–Ω—É—î —à–ª—è—Ö\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"–®–ª—è—Ö {data_path} –Ω–µ —ñ—Å–Ω—É—î\")\n",
    "            \n",
    "        full_dataset = ImageFolder(root=data_path, transform=transform)\n",
    "        print(f\" –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(full_dataset)} –∑–æ–±—Ä–∞–∂–µ–Ω—å\")\n",
    "        print(f\"   –ö–ª–∞—Å–∏: {full_dataset.classes}\")\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º–æ –ª–∏—à–µ –Ω–µ–≤–µ–ª–∏–∫—É –≤–∏–±—ñ—Ä–∫—É –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ\n",
    "        total_samples = min(SUBSET_SIZE + TEST_SIZE, len(full_dataset))\n",
    "        indices = torch.randperm(len(full_dataset))[:total_samples]\n",
    "        \n",
    "        train_size = SUBSET_SIZE\n",
    "        train_indices = indices[:train_size]\n",
    "        test_indices = indices[train_size:train_size + TEST_SIZE]\n",
    "        \n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        test_dataset = Subset(full_dataset, test_indices)\n",
    "        \n",
    "        print(f\"üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ: {len(train_dataset)} —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö, {len(test_dataset)} —Ç–µ—Å—Ç–æ–≤–∏—Ö\")\n",
    "        \n",
    "        return train_dataset, test_dataset, full_dataset.classes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}\")\n",
    "        print(\"–°—Ç–≤–æ—Ä—é—î–º–æ –¥–µ–º–æ-–¥–∞–Ω—ñ...\")\n",
    "        from torchvision.datasets import FakeData\n",
    "        train_dataset = FakeData(size=SUBSET_SIZE, image_size=(3, 64, 64), num_classes=3, transform=transforms.ToTensor())\n",
    "        test_dataset = FakeData(size=TEST_SIZE, image_size=(3, 64, 64), num_classes=3, transform=transforms.ToTensor())\n",
    "        class_names = ['cat', 'dog', 'wild']\n",
    "        \n",
    "        print(f\"üìä –î–µ–º–æ-–¥–∞–Ω—ñ: {len(train_dataset)} —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö, {len(test_dataset)} —Ç–µ—Å—Ç–æ–≤–∏—Ö\")\n",
    "        \n",
    "        return train_dataset, test_dataset, class_names\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ\n",
    "DATA_PATH = \"D:/Kotopes/Kotopes/data\"\n",
    "print(f\" –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑: {DATA_PATH}\")\n",
    "train_dataset, test_dataset, class_names = load_tiny_dataset(DATA_PATH)\n",
    "\n",
    "# --- Cell 3: –¢–†–ï–ù–£–í–ê–ù–ù–Ø ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\" –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}\")\n",
    "\n",
    "model = FastAutoencoder(latent_dim=LATENT_DIM).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def fast_train(model, train_loader, test_loader, epochs=EPOCHS):\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss: {train_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "print(\"–ü–æ—á–∞—Ç–æ–∫ —à–≤–∏–¥–∫–æ–≥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...\")\n",
    "train_losses = fast_train(model, train_loader, test_loader)\n",
    "\n",
    "# --- Cell 4: –®–í–ò–î–ö–ò–ô –ê–ù–ê–õ–Ü–ó (–í–ò–ü–†–ê–í–õ–ï–ù–ê –í–ï–†–°–Ü–Ø) ---\n",
    "print(\"\\n=== –ê–ù–ê–õ–Ü–ó ===\")\n",
    "\n",
    "# –í–∏–∑–Ω–∞—á–∞—î–º–æ –Ω–æ—Ä–º–∞–ª—å–Ω–∏–π –∫–ª–∞—Å\n",
    "NORMAL_CLASS = class_names.index('cat') if 'cat' in class_names else 0\n",
    "print(f\"–ù–æ—Ä–º–∞–ª—å–Ω–∏–π –∫–ª–∞—Å: {class_names[NORMAL_CLASS]}\")\n",
    "\n",
    "# –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ (–í–ò–ü–†–ê–í–õ–ï–ù–ê –ß–ê–°–¢–ò–ù–ê)\n",
    "test_normal_indices = []\n",
    "test_anomalous_indices = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    img, label = test_dataset[i]  # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ - –±–µ–∑ –∑–∞–π–≤–æ–≥–æ unpacking\n",
    "    if label == NORMAL_CLASS:\n",
    "        test_normal_indices.append(i)\n",
    "    else:\n",
    "        test_anomalous_indices.append(i)\n",
    "\n",
    "# –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ\n",
    "test_normal_indices = test_normal_indices[:20]\n",
    "test_anomalous_indices = test_anomalous_indices[:20]\n",
    "\n",
    "test_normal = Subset(test_dataset, test_normal_indices)\n",
    "test_anomalous = Subset(test_dataset, test_anomalous_indices)\n",
    "\n",
    "print(f\" –¢–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ: {len(test_normal)} –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö, {len(test_anomalous)} –∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫\n",
    "def fast_errors(model, dataset, device):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            reconstructed = model(images)\n",
    "            error = torch.mean((images - reconstructed) ** 2, dim=[1,2,3])\n",
    "            errors.extend(error.cpu().numpy())\n",
    "    return np.array(errors)\n",
    "\n",
    "normal_errors = fast_errors(model, test_normal, device)\n",
    "anomalous_errors = fast_errors(model, test_anomalous, device)\n",
    "\n",
    "threshold = np.percentile(normal_errors, 95)\n",
    "detection_rate = np.mean(anomalous_errors > threshold) * 100\n",
    "false_positive = np.mean(normal_errors > threshold) * 100\n",
    "\n",
    "print(f\"–†–ï–ó–£–õ–¨–¢–ê–¢–ò:\")\n",
    "print(f\"   - Threshold: {threshold:.4f}\")\n",
    "print(f\"   - –í–∏—è–≤–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª—ñ–π: {detection_rate:.1f}%\")\n",
    "print(f\"   - –ü–æ–º–∏–ª–∫–∞ (FPR): {false_positive:.1f}%\")\n",
    "\n",
    "# --- Cell 5: –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø ---\n",
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "# –ì—Ä–∞—Ñ—ñ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('–í—Ç—Ä–∞—Ç–∏ –ø—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ')\n",
    "plt.xlabel('–ï–ø–æ—Ö–∞')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(normal_errors, alpha=0.7, label='–ù–æ—Ä–º–∞–ª—å–Ω—ñ', bins=15, color='green')\n",
    "plt.hist(anomalous_errors, alpha=0.7, label='–ê–Ω–æ–º–∞–ª—å–Ω—ñ', bins=15, color='red')\n",
    "plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold: {threshold:.3f}')\n",
    "plt.legend()\n",
    "plt.title('–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –ü—Ä–∏–∫–ª–∞–¥–∏\n",
    "plt.subplot(1, 3, 3)\n",
    "# –ë–µ—Ä–µ–º–æ –æ–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó\n",
    "sample_idx = 0\n",
    "sample_img, sample_label = test_dataset[sample_idx]  # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ\n",
    "sample_img = sample_img.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(sample_img)\n",
    "\n",
    "plt.imshow(sample_img[0].cpu().permute(1, 2, 0))\n",
    "plt.title(f'–ü—Ä–∏–∫–ª–∞–¥: {class_names[sample_label]}')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Cell 6: –õ–ê–¢–ï–ù–¢–ù–ò–ô –ü–†–û–°–¢–Ü–† ---\n",
    "print(\"\\n=== –õ–ê–¢–ï–ù–¢–ù–ò–ô –ü–†–û–°–¢–Ü–† ===\")\n",
    "\n",
    "def fast_latent_vectors(model, dataset, device, max_samples=50):\n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "        for images, batch_labels in loader:\n",
    "            images = images.to(device)\n",
    "            latent = model.encoder(images)\n",
    "            latent_vectors.extend(latent.cpu().numpy())\n",
    "            labels.extend(batch_labels.numpy())\n",
    "            \n",
    "            if len(latent_vectors) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    return np.array(latent_vectors)[:max_samples], np.array(labels)[:max_samples]\n",
    "\n",
    "latent_vectors, labels = fast_latent_vectors(model, test_dataset, device)\n",
    "\n",
    "print(f\" –í–∏—Ç—è–≥–Ω—É—Ç–æ {len(latent_vectors)} –ª–∞—Ç–µ–Ω—Ç–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä—ñ–≤\")\n",
    "\n",
    "# t-SNE –¥–ª—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç–æ—Ä—É\n",
    "if len(latent_vectors) > 10:\n",
    "    print(\"üîç –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, len(latent_vectors)-1))\n",
    "    latent_2d = tsne.fit_transform(latent_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # –ó–∞ –∫–ª–∞—Å–∞–º–∏\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for class_idx in range(len(class_names)):\n",
    "        mask = labels == class_idx\n",
    "        if np.sum(mask) > 0:\n",
    "            plt.scatter(latent_2d[mask, 0], latent_2d[mask, 1], \n",
    "                       label=class_names[class_idx], alpha=0.7, s=30)\n",
    "    plt.legend()\n",
    "    plt.title('–õ–∞—Ç–µ–Ω—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä –∑–∞ –∫–ª–∞—Å–∞–º–∏')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª—å–Ω—ñ vs –∞–Ω–æ–º–∞–ª—å–Ω—ñ\n",
    "    plt.subplot(1, 2, 2)\n",
    "    is_normal = (labels == NORMAL_CLASS).astype(int)\n",
    "    plt.scatter(latent_2d[is_normal==1, 0], latent_2d[is_normal==1, 1], \n",
    "               c='green', label=f'–ù–æ—Ä–º–∞–ª—å–Ω—ñ ({class_names[NORMAL_CLASS]})', alpha=0.7, s=30)\n",
    "    plt.scatter(latent_2d[is_normal==0, 0], latent_2d[is_normal==0, 1], \n",
    "               c='red', label='–ê–Ω–æ–º–∞–ª—å–Ω—ñ', alpha=0.7, s=30)\n",
    "    plt.legend()\n",
    "    plt.title('–ü–æ–¥—ñ–ª –Ω–æ—Ä–º–∞–ª—å–Ω–∏—Ö/–∞–Ω–æ–º–∞–ª—å–Ω–∏—Ö')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\" –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è t-SNE\")\n",
    "\n",
    "print(\"\\n–ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "print(f\" –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ: {SUBSET_SIZE} —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö —Ç–∞ {TEST_SIZE} —Ç–µ—Å—Ç–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å\")\n",
    "print(f\" –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö: {EPOCHS}\")\n",
    "print(f\" –ö–ª–∞—Å–∏: {class_names}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
