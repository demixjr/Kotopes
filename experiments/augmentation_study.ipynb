{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce2591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Results directory: ../results/augmentation_study\n",
      "============================================================\n",
      "EXPERIMENT 1: Augmentation Comparison\n",
      "============================================================\n",
      "\n",
      "--- Testing BASELINE augmentation ---\n",
      "Augmentation: baseline\n",
      "Training samples: 14630\n",
      "Validation samples: 1500\n",
      "Classes: ['cat', 'dog', 'wild']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 1/10: 100%|██████████| 1829/1829 [03:31<00:00,  8.63it/s, Loss=0.4693, Acc=73.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.6202, Train Acc: 73.67%, Val Loss: 0.3004, Val Acc: 89.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 2/10: 100%|██████████| 1829/1829 [03:35<00:00,  8.49it/s, Loss=0.0675, Acc=88.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.3150, Train Acc: 88.25%, Val Loss: 0.2231, Val Acc: 91.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 3/10: 100%|██████████| 1829/1829 [03:37<00:00,  8.40it/s, Loss=0.2724, Acc=91.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2287, Train Acc: 91.63%, Val Loss: 0.1629, Val Acc: 94.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 4/10: 100%|██████████| 1829/1829 [03:34<00:00,  8.52it/s, Loss=0.6009, Acc=92.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.1904, Train Acc: 92.93%, Val Loss: 0.1422, Val Acc: 95.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 5/10: 100%|██████████| 1829/1829 [03:33<00:00,  8.55it/s, Loss=0.7301, Acc=94.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.1644, Train Acc: 94.21%, Val Loss: 0.1288, Val Acc: 95.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 6/10: 100%|██████████| 1829/1829 [03:35<00:00,  8.48it/s, Loss=0.0922, Acc=94.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.1465, Train Acc: 94.61%, Val Loss: 0.1124, Val Acc: 96.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aug_baseline Epoch 7/10:  25%|██▌       | 466/1829 [01:01<03:00,  7.56it/s, Loss=0.0052, Acc=95.25%] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 868\u001b[39m\n\u001b[32m    864\u001b[39m study = AugmentationStudy(data_dir=\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    867\u001b[39m     \u001b[38;5;66;03m# Запуск експериментів\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperiment_1_augmentation_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m     study.experiment_2_regularization_methods()\n\u001b[32m    870\u001b[39m     study.experiment_3_ensemble_models()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 325\u001b[39m, in \u001b[36mAugmentationStudy.experiment_1_augmentation_comparison\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    322\u001b[39m optimizer = torch.optim.SGD(model.parameters(), lr=\u001b[32m0.0005\u001b[39m, momentum=\u001b[32m0.9\u001b[39m)\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Навчання\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m model_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maug_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maug_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcnn_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maug_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_config\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Зберігаємо результати\u001b[39;00m\n\u001b[32m    334\u001b[39m model_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maug_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maug_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mAugmentationStudy.train_and_evaluate\u001b[39m\u001b[34m(self, model, train_loader, val_loader, criterion, optimizer, epochs, experiment_name, model_name, experiment_config)\u001b[39m\n\u001b[32m    149\u001b[39m outputs = model(images)\n\u001b[32m    150\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m optimizer.step()\n\u001b[32m    154\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\k\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\k\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\k\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# experiments/augmentation_study.ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "\n",
    "# Додаємо шляхи\n",
    "sys.path.append('..')\n",
    "from models.custom_cnn import CustomCNN\n",
    "from models.transfer_models import get_model\n",
    "from utils.data_loader import get_data_loaders\n",
    "from augmentation.baseline_aug import get_baseline_transforms\n",
    "from augmentation.advanced_aug import get_advanced_transforms, get_baseline_transforms\n",
    "from utils.regularization import RegularizationTechniques\n",
    "\n",
    "class AugmentationStudy:\n",
    "    def __init__(self, data_dir='../data', results_dir='../results/augmentation_study'):\n",
    "        self.data_dir = data_dir\n",
    "        self.results_dir = results_dir\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.training_history = {}  # Зберігає історію навчання для кожної моделі\n",
    "        self.trained_models = {}  # Зберігатиме навчені моделі для ensemble\n",
    "        \n",
    "        # Створюємо папки для результатів\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(results_dir, 'training_curves'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(results_dir, 'model_checkpoints'), exist_ok=True)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"Results directory: {self.results_dir}\")\n",
    "        \n",
    "        # Завантажуємо попередні результати, якщо вони є\n",
    "        self._load_previous_results()\n",
    "    \n",
    "    def _load_previous_results(self):\n",
    "        \"\"\"Завантаження попередніх результатів\"\"\"\n",
    "        history_files = [f for f in os.listdir(self.results_dir) if f.startswith('training_history_') and f.endswith('.json')]\n",
    "        if history_files:\n",
    "            # Беремо найновіший файл\n",
    "            latest_file = sorted(history_files)[-1]\n",
    "            history_path = os.path.join(self.results_dir, latest_file)\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    self.training_history = json.load(f)\n",
    "                print(f\"Loaded previous results from: {latest_file}\")\n",
    "                print(f\"Found {len(self.training_history)} trained models\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load previous results: {e}\")\n",
    "    \n",
    "    def _get_experiment_hash(self, experiment_config):\n",
    "        \"\"\"Генерує унікальний хеш для експерименту\"\"\"\n",
    "        config_str = json.dumps(experiment_config, sort_keys=True, default=str)\n",
    "        return hashlib.md5(config_str.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def _is_experiment_done(self, experiment_name, experiment_config):\n",
    "        \"\"\"Перевіряє, чи вже був проведений цей експеримент\"\"\"\n",
    "        experiment_hash = self._get_experiment_hash(experiment_config)\n",
    "        \n",
    "        # Перевіряємо в історії\n",
    "        if experiment_name in self.training_history:\n",
    "            stored_hash = self.training_history[experiment_name].get('experiment_hash')\n",
    "            if stored_hash == experiment_hash:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def setup_data_augmentation(self, augmentation_type='baseline', image_size=64, batch_size=16):\n",
    "        \"\"\"Завантаження даних з різними аугментаціями\"\"\"\n",
    "        from torchvision import datasets, transforms\n",
    "        from torch.utils.data import DataLoader\n",
    "        \n",
    "        if augmentation_type == 'baseline':\n",
    "            transform_dict = get_baseline_transforms(image_size)\n",
    "            train_transform = transform_dict['train']\n",
    "        elif augmentation_type == 'advanced':\n",
    "            try:\n",
    "                transform_dict = get_advanced_transforms(image_size)\n",
    "                train_transform = transform_dict['train']\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Advanced augmentation failed, using baseline: {e}\")\n",
    "                transform_dict = get_baseline_transforms(image_size)\n",
    "                train_transform = transform_dict['train']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown augmentation type: {augmentation_type}\")\n",
    "        \n",
    "        # Валідаційні трансформації завжди однакові\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Завантаження даних\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, 'train'),\n",
    "            transform=train_transform\n",
    "        )\n",
    "        val_dataset = datasets.ImageFolder(\n",
    "            os.path.join(self.data_dir, 'val'),\n",
    "            transform=val_transform\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        class_names = train_dataset.classes\n",
    "        print(f\"Augmentation: {augmentation_type}\")\n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        print(f\"Classes: {class_names}\")\n",
    "        \n",
    "        return train_loader, val_loader, class_names\n",
    "\n",
    "    def train_and_evaluate(self, model, train_loader, val_loader, criterion, optimizer, \n",
    "                          epochs=10, experiment_name=\"exp\", model_name=\"model\", \n",
    "                          experiment_config=None):\n",
    "        \"\"\"Навчання та оцінка моделі\"\"\"\n",
    "        \n",
    "        # Перевіряємо, чи вже був проведений цей експеримент\n",
    "        if experiment_config and self._is_experiment_done(experiment_name, experiment_config):\n",
    "            print(f\"Experiment {experiment_name} already completed. Skipping...\")\n",
    "            return self.training_history[experiment_name]\n",
    "        \n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'{experiment_name} Epoch {epoch+1}/{epochs}')\n",
    "            for images, labels in pbar:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "                })\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = 100.0 * train_correct / train_total\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            print(f'Epoch {epoch+1}: '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Генеруємо графіки для цієї моделі\n",
    "        self._plot_single_training_curve(experiment_name, train_losses, val_losses, train_accs, val_accs, epochs)\n",
    "        \n",
    "        # Зберігаємо історію навчання для цієї моделі\n",
    "        training_history = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_accs': val_accs,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'epochs': epochs,\n",
    "            'model_name': model_name,\n",
    "            'experiment_name': experiment_name,\n",
    "            'timestamp': datetime.datetime.now().isoformat(),\n",
    "            'experiment_hash': self._get_experiment_hash(experiment_config) if experiment_config else None\n",
    "        }\n",
    "        \n",
    "        # Зберігаємо модель\n",
    "        model_info = {\n",
    "            'model': model,\n",
    "            'state_dict': best_model_state,\n",
    "            'training_history': training_history\n",
    "        }\n",
    "        \n",
    "        # Оновлюємо глобальну історію\n",
    "        self.training_history[experiment_name] = training_history\n",
    "        \n",
    "        # Зберігаємо результати після кожної тренування\n",
    "        self._save_training_history()\n",
    "        \n",
    "        return model_info\n",
    "\n",
    "    def _plot_single_training_curve(self, experiment_name, train_losses, val_losses, train_accs, val_accs, epochs):\n",
    "        \"\"\"Генерує графіки для однієї тренування\"\"\"\n",
    "        if len(train_losses) == 0:\n",
    "            return\n",
    "            \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        epoch_range = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        # Loss curves\n",
    "        ax1.plot(epoch_range, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "        ax1.plot(epoch_range, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "        ax1.set_title(f'{experiment_name} - Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        ax2.plot(epoch_range, train_accs, 'b-', label='Train Acc', linewidth=2)\n",
    "        ax2.plot(epoch_range, val_accs, 'r-', label='Val Acc', linewidth=2)\n",
    "        ax2.set_title(f'{experiment_name} - Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Final metrics\n",
    "        metrics = ['Final Train Acc', 'Final Val Acc', 'Best Val Acc']\n",
    "        values = [train_accs[-1], val_accs[-1], max(val_accs)]\n",
    "        \n",
    "        bars = ax3.bar(metrics, values, color=['lightblue', 'lightcoral', 'lightgreen'])\n",
    "        ax3.set_title(f'{experiment_name} - Final Metrics')\n",
    "        ax3.set_ylabel('Accuracy (%)')\n",
    "        ax3.set_ylim(0, max(values) * 1.1)\n",
    "        for bar, v in zip(bars, values):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, v + 0.5, f'{v:.2f}%', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Training summary\n",
    "        ax4.axis('off')\n",
    "        summary_text = (\n",
    "            f\"Model: {experiment_name}\\n\"\n",
    "            f\"Epochs: {epochs}\\n\"\n",
    "            f\"Best Val Acc: {max(val_accs):.2f}%\\n\"\n",
    "            f\"Final Train Acc: {train_accs[-1]:.2f}%\\n\"\n",
    "            f\"Final Val Acc: {val_accs[-1]:.2f}%\\n\"\n",
    "            f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        )\n",
    "        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.results_dir}/training_curves/{experiment_name}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Training curves saved for: {experiment_name}\")\n",
    "\n",
    "    def experiment_1_augmentation_comparison(self):\n",
    "        \"\"\"Експеримент 1: Порівняння різних аугментацій\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"EXPERIMENT 1: Augmentation Comparison\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        augmentation_types = ['baseline', 'advanced']\n",
    "        \n",
    "        for aug_type in augmentation_types:\n",
    "            print(f\"\\n--- Testing {aug_type.upper()} augmentation ---\")\n",
    "            \n",
    "            # Завантаження даних\n",
    "            train_loader, val_loader, class_names = self.setup_data_augmentation(\n",
    "                augmentation_type=aug_type, \n",
    "                image_size=64, \n",
    "                batch_size=8\n",
    "            )\n",
    "            \n",
    "            # Конфігурація експерименту\n",
    "            experiment_config = {\n",
    "                'experiment_type': 'augmentation_comparison',\n",
    "                'augmentation_type': aug_type,\n",
    "                'image_size': 64,\n",
    "                'batch_size': 8,\n",
    "                'epochs': 10,\n",
    "                'model_type': 'CustomCNN',\n",
    "                'num_classes': len(class_names)\n",
    "            }\n",
    "            \n",
    "            # Створення моделі\n",
    "            model = CustomCNN(num_classes=len(class_names), input_size=64).to(self.device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "            \n",
    "            # Навчання\n",
    "            model_info = self.train_and_evaluate(\n",
    "                model, train_loader, val_loader, criterion, optimizer,\n",
    "                epochs=10, \n",
    "                experiment_name=f\"aug_{aug_type}\", \n",
    "                model_name=f\"cnn_{aug_type}\",\n",
    "                experiment_config=experiment_config\n",
    "            )\n",
    "            \n",
    "            # Зберігаємо результати\n",
    "            model_name = f\"aug_{aug_type}\"\n",
    "            self.trained_models[model_name] = model_info\n",
    "            \n",
    "            print(f\"Best Validation Accuracy: {model_info['training_history']['best_val_acc']:.2f}%\")\n",
    "\n",
    "    def experiment_2_regularization_methods(self):\n",
    "        \"\"\"Експеримент 2: Порівняння методів регуляризації\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXPERIMENT 2: Regularization Methods\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Завантаження даних (фіксовані для порівняння)\n",
    "        train_loader, val_loader, class_names = self.setup_data_augmentation(\n",
    "            augmentation_type='baseline', \n",
    "            image_size=64, \n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        regularization_configs = {\n",
    "            'baseline': {\n",
    "                'criterion': nn.CrossEntropyLoss(),\n",
    "                'optimizer': lambda m: torch.optim.SGD(m.parameters(), lr=0.0005, momentum=0.9),\n",
    "                'model_config': {'dropout_rate': 0.3, 'use_batchnorm': True}\n",
    "            },\n",
    "            'high_dropout': {\n",
    "                'criterion': nn.CrossEntropyLoss(),\n",
    "                'optimizer': lambda m: torch.optim.SGD(m.parameters(), lr=0.0005, momentum=0.9),\n",
    "                'model_config': {'dropout_rate': 0.7, 'use_batchnorm': True}\n",
    "            },\n",
    "            'l2_regularization': {\n",
    "                'criterion': nn.CrossEntropyLoss(),\n",
    "                'optimizer': lambda m: torch.optim.SGD(m.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4),\n",
    "                'model_config': {'dropout_rate': 0.3, 'use_batchnorm': True}\n",
    "            },\n",
    "            'label_smoothing': {\n",
    "                'criterion': lambda outputs, targets: self.label_smoothing_loss(outputs, targets, smoothing=0.1),\n",
    "                'optimizer': lambda m: torch.optim.SGD(m.parameters(), lr=0.0005, momentum=0.9),\n",
    "                'model_config': {'dropout_rate': 0.3, 'use_batchnorm': True}\n",
    "            },\n",
    "            'combined': {\n",
    "                'criterion': lambda outputs, targets: self.label_smoothing_loss(outputs, targets, smoothing=0.1),\n",
    "                'optimizer': lambda m: torch.optim.SGD(m.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4),\n",
    "                'model_config': {'dropout_rate': 0.5, 'use_batchnorm': True}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for reg_name, config in regularization_configs.items():\n",
    "            print(f\"\\n--- Testing {reg_name.upper()} ---\")\n",
    "            \n",
    "            # Конфігурація експерименту\n",
    "            experiment_config = {\n",
    "                'experiment_type': 'regularization_study',\n",
    "                'regularization_method': reg_name,\n",
    "                'model_config': config['model_config'],\n",
    "                'image_size': 64,\n",
    "                'batch_size': 8,\n",
    "                'epochs': 10\n",
    "            }\n",
    "            \n",
    "            # Створення моделі\n",
    "            model = CustomCNN(\n",
    "                num_classes=len(class_names), \n",
    "                input_size=64,\n",
    "                **config['model_config']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            criterion = config['criterion']\n",
    "            optimizer = config['optimizer'](model)\n",
    "            \n",
    "            # Навчання\n",
    "            model_info = self.train_and_evaluate(\n",
    "                model, train_loader, val_loader, criterion, optimizer,\n",
    "                epochs=10, \n",
    "                experiment_name=f\"reg_{reg_name}\", \n",
    "                model_name=f\"cnn_{reg_name}\",\n",
    "                experiment_config=experiment_config\n",
    "            )\n",
    "            \n",
    "            # Зберігаємо результати\n",
    "            model_name = f\"reg_{reg_name}\"\n",
    "            self.trained_models[model_name] = model_info\n",
    "            \n",
    "            print(f\"Best Validation Accuracy: {model_info['training_history']['best_val_acc']:.2f}%\")\n",
    "    \n",
    "    def label_smoothing_loss(self, outputs, targets, smoothing=0.1):\n",
    "        \"\"\"Label Smoothing Loss\"\"\"\n",
    "        log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=targets.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = (1 - smoothing) * nll_loss + smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "    def experiment_3_ensemble_models(self):\n",
    "        \"\"\"Експеримент 3: Ensemble моделей\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXPERIMENT 3: Ensemble Models\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Завантаження даних\n",
    "        train_loader, val_loader, class_names = self.setup_data_augmentation(\n",
    "            augmentation_type='baseline',\n",
    "            image_size=64,\n",
    "            batch_size=8\n",
    "        )\n",
    "        \n",
    "        # 1. Навчання різних базових моделей для ансамбля\n",
    "        ensemble_models = {}\n",
    "        \n",
    "        # Різні архітектури для ансамбля\n",
    "        model_configs = {\n",
    "            'cnn_small': {'num_layers': 3, 'dropout_rate': 0.3},\n",
    "            'cnn_medium': {'num_layers': 4, 'dropout_rate': 0.4},\n",
    "            'cnn_large': {'num_layers': 5, 'dropout_rate': 0.5},\n",
    "        }\n",
    "        \n",
    "        for model_name, config in model_configs.items():\n",
    "            print(f\"\\n--- Training {model_name.upper()} for ensemble ---\")\n",
    "            \n",
    "            # Конфігурація експерименту\n",
    "            experiment_config = {\n",
    "                'experiment_type': 'ensemble_training',\n",
    "                'ensemble_model': model_name,\n",
    "                'model_config': config,\n",
    "                'image_size': 64,\n",
    "                'batch_size': 8,\n",
    "                'epochs': 8\n",
    "            }\n",
    "            \n",
    "            # Custom CNN models\n",
    "            model = CustomCNN(\n",
    "                num_classes=len(class_names),\n",
    "                input_size=64,\n",
    "                dropout_rate=config['dropout_rate'],\n",
    "                num_conv_layers=config['num_layers']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "            \n",
    "            # Коротке навчання для ансамбля\n",
    "            model_info = self.train_and_evaluate(\n",
    "                model, train_loader, val_loader, criterion, optimizer,\n",
    "                epochs=8, \n",
    "                experiment_name=f\"ensemble_{model_name}\", \n",
    "                model_name=model_name,\n",
    "                experiment_config=experiment_config\n",
    "            )\n",
    "            \n",
    "            ensemble_models[model_name] = model_info\n",
    "        \n",
    "        # 2. Тестування різних методів ансамблювання\n",
    "        ensemble_methods = {\n",
    "            'average': self.average_ensemble,\n",
    "            'weighted_average': self.weighted_average_ensemble,\n",
    "            'voting': self.majority_voting_ensemble\n",
    "        }\n",
    "        \n",
    "        for method_name, ensemble_func in ensemble_methods.items():\n",
    "            print(f\"\\n--- Testing {method_name.upper()} ensemble ---\")\n",
    "            \n",
    "            ensemble_accuracy = ensemble_func(ensemble_models, val_loader)\n",
    "            \n",
    "            # Конфігурація експерименту ансамбля\n",
    "            experiment_config = {\n",
    "                'experiment_type': 'ensemble_method',\n",
    "                'ensemble_method': method_name,\n",
    "                'base_models': list(ensemble_models.keys())\n",
    "            }\n",
    "            \n",
    "            # Зберігаємо результати ансамбля\n",
    "            ensemble_history = {\n",
    "                'best_val_acc': ensemble_accuracy,\n",
    "                'train_accs': [ensemble_accuracy],\n",
    "                'val_accs': [ensemble_accuracy],\n",
    "                'train_losses': [0],\n",
    "                'val_losses': [0],\n",
    "                'epochs': 1,\n",
    "                'model_name': f'ensemble_{method_name}',\n",
    "                'experiment_name': 'ensemble_study',\n",
    "                'timestamp': datetime.datetime.now().isoformat(),\n",
    "                'experiment_hash': self._get_experiment_hash(experiment_config)\n",
    "            }\n",
    "            \n",
    "            self.training_history[f'ensemble_{method_name}'] = ensemble_history\n",
    "            \n",
    "            print(f\"Ensemble Accuracy ({method_name}): {ensemble_accuracy:.2f}%\")\n",
    "        \n",
    "        # 3. Порівняння з найкращою індивідуальною моделлю\n",
    "        individual_results = [history['best_val_acc'] for name, history in self.training_history.items() \n",
    "                            if name.startswith('ensemble_') and not any(m in name for m in ensemble_methods.keys())]\n",
    "        \n",
    "        if individual_results:\n",
    "            best_individual = max(individual_results)\n",
    "            best_ensemble = max([history['best_val_acc'] for name, history in self.training_history.items() \n",
    "                               if any(m in name for m in ensemble_methods.keys())])\n",
    "            \n",
    "            print(f\"\\nEnsemble vs Individual Comparison:\")\n",
    "            print(f\"   Best Individual Model: {best_individual:.2f}%\")\n",
    "            print(f\"   Best Ensemble Method: {best_ensemble:.2f}%\")\n",
    "            print(f\"   Improvement: {best_ensemble - best_individual:+.2f}%\")\n",
    "        \n",
    "        # Зберігаємо результати після ансамблювання\n",
    "        self._save_training_history()\n",
    "\n",
    "    def average_ensemble(self, models_dict, data_loader):\n",
    "        \"\"\"Ensemble метод: просте усереднення прогнозів\"\"\"\n",
    "        models = [data['model'] for data in models_dict.values()]\n",
    "        \n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Збираємо прогнози від усіх моделей\n",
    "                all_predictions = []\n",
    "                for model in models:\n",
    "                    outputs = model(images)\n",
    "                    probabilities = F.softmax(outputs, dim=1)\n",
    "                    all_predictions.append(probabilities)\n",
    "                \n",
    "                # Усереднення прогнозів\n",
    "                avg_predictions = torch.mean(torch.stack(all_predictions), dim=0)\n",
    "                _, predicted = torch.max(avg_predictions, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100.0 * correct / total\n",
    "\n",
    "    def weighted_average_ensemble(self, models_dict, data_loader):\n",
    "        \"\"\"Ensemble метод: зважене усереднення на основі точності моделей\"\"\"\n",
    "        models = list(models_dict.keys())\n",
    "        model_instances = [models_dict[name]['model'] for name in models]\n",
    "        model_accuracies = [models_dict[name]['training_history']['best_val_acc'] for name in models]\n",
    "        \n",
    "        # Нормалізуємо ваги\n",
    "        weights = torch.tensor(model_accuracies, device=self.device)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        for model in model_instances:\n",
    "            model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Збираємо прогнози від усіх моделей\n",
    "                all_predictions = []\n",
    "                for model in model_instances:\n",
    "                    outputs = model(images)\n",
    "                    probabilities = F.softmax(outputs, dim=1)\n",
    "                    all_predictions.append(probabilities)\n",
    "                \n",
    "                # Зважене усереднення\n",
    "                weighted_predictions = torch.zeros_like(all_predictions[0])\n",
    "                for i, pred in enumerate(all_predictions):\n",
    "                    weighted_predictions += weights[i] * pred\n",
    "                \n",
    "                _, predicted = torch.max(weighted_predictions, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100.0 * correct / total\n",
    "\n",
    "    def majority_voting_ensemble(self, models_dict, data_loader):\n",
    "        \"\"\"Ensemble метод: мажоритарне голосування\"\"\"\n",
    "        models = [data['model'] for data in models_dict.values()]\n",
    "        \n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Збираємо прогнози від усіх моделей\n",
    "                all_predictions = []\n",
    "                for model in models:\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    all_predictions.append(predicted)\n",
    "                \n",
    "                # Мажоритарне голосування\n",
    "                predictions_stack = torch.stack(all_predictions)\n",
    "                final_predictions = torch.mode(predictions_stack, dim=0).values\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (final_predictions == labels).sum().item()\n",
    "        \n",
    "        return 100.0 * correct / total\n",
    "\n",
    "    def _save_training_history(self):\n",
    "        \"\"\"Зберігає історію тренувань у файл\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        history_filename = f'{self.results_dir}/training_history_{timestamp}.json'\n",
    "        \n",
    "        # Конвертуємо дані для JSON\n",
    "        serializable_history = {}\n",
    "        for key, history in self.training_history.items():\n",
    "            serializable_history[key] = {\n",
    "                'train_losses': [float(x) for x in history['train_losses']],\n",
    "                'val_losses': [float(x) for x in history['val_losses']],\n",
    "                'train_accs': [float(x) for x in history['train_accs']],\n",
    "                'val_accs': [float(x) for x in history['val_accs']],\n",
    "                'best_val_acc': float(history['best_val_acc']),\n",
    "                'epochs': history['epochs'],\n",
    "                'model_name': history['model_name'],\n",
    "                'experiment_name': history['experiment_name'],\n",
    "                'timestamp': history['timestamp'],\n",
    "                'experiment_hash': history.get('experiment_hash')\n",
    "            }\n",
    "        \n",
    "        with open(history_filename, 'w') as f:\n",
    "            json.dump(serializable_history, f, indent=2)\n",
    "        \n",
    "        print(f\"Training history saved to: {history_filename}\")\n",
    "\n",
    "    def plot_comparison_results(self):\n",
    "        \"\"\"Побудова графіків порівняння результатів\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"No training history available for plotting\")\n",
    "            return\n",
    "        \n",
    "        # 1. Порівняння аугментацій\n",
    "        self._plot_augmentation_comparison()\n",
    "        \n",
    "        # 2. Порівняння регуляризацій\n",
    "        self._plot_regularization_comparison()\n",
    "        \n",
    "        # 3. Порівняння Ensemble методів\n",
    "        self._plot_ensemble_comparison()\n",
    "        \n",
    "        # 4. Загальне порівняння\n",
    "        self._plot_overall_comparison()\n",
    "\n",
    "    def _plot_augmentation_comparison(self):\n",
    "        \"\"\"Графік порівняння аугментацій\"\"\"\n",
    "        aug_history = {k: v for k, v in self.training_history.items() if k.startswith('aug_')}\n",
    "        \n",
    "        if not aug_history:\n",
    "            return\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        for name, history in aug_history.items():\n",
    "            aug_name = name.replace('aug_', '').replace('_', ' ').title()\n",
    "            epochs = range(1, len(history['val_accs']) + 1)\n",
    "            \n",
    "            ax1.plot(epochs, history['val_losses'], label=aug_name, linewidth=2)\n",
    "            ax2.plot(epochs, history['val_accs'], label=aug_name, linewidth=2)\n",
    "        \n",
    "        ax1.set_title('Augmentation Comparison - Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.set_title('Augmentation Comparison - Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.results_dir}/augmentation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_regularization_comparison(self):\n",
    "        \"\"\"Графік порівняння методів регуляризації\"\"\"\n",
    "        reg_history = {k: v for k, v in self.training_history.items() if k.startswith('reg_')}\n",
    "        \n",
    "        if not reg_history:\n",
    "            return\n",
    "            \n",
    "        methods = [name.replace('reg_', '').replace('_', ' ').title() for name in reg_history.keys()]\n",
    "        accuracies = [history['best_val_acc'] for history in reg_history.values()]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, accuracies, color=['#FF9999', '#66B2FF', '#99FF99', '#FFD700', '#FF99CC'])\n",
    "        \n",
    "        plt.title('Regularization Methods Comparison')\n",
    "        plt.ylabel('Best Validation Accuracy (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, max(accuracies) * 1.1 if accuracies else 100)\n",
    "        \n",
    "        # Додаємо значення на стовпці\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.results_dir}/regularization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_ensemble_comparison(self):\n",
    "        \"\"\"Графік порівняння ensemble методів\"\"\"\n",
    "        ensemble_history = {k: v for k, v in self.training_history.items() if 'ensemble_' in k}\n",
    "        \n",
    "        if not ensemble_history:\n",
    "            return\n",
    "            \n",
    "        # Розділяємо індивідуальні моделі та ensemble методи\n",
    "        individual_models = {k: v for k, v in ensemble_history.items() \n",
    "                           if not any(m in k for m in ['average', 'weighted', 'voting'])}\n",
    "        ensemble_methods = {k: v for k, v in ensemble_history.items() \n",
    "                          if any(m in k for m in ['average', 'weighted', 'voting'])}\n",
    "        \n",
    "        if not individual_models or not ensemble_methods:\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Графік індивідуальних моделей\n",
    "        model_names = [name.replace('ensemble_', '').upper() for name in individual_models.keys()]\n",
    "        model_accuracies = [history['best_val_acc'] for history in individual_models.values()]\n",
    "        \n",
    "        bars1 = ax1.bar(model_names, model_accuracies, color='skyblue', alpha=0.7)\n",
    "        ax1.set_title('Individual Models Performance')\n",
    "        ax1.set_ylabel('Validation Accuracy (%)')\n",
    "        ax1.set_xticklabels(model_names, rotation=45)\n",
    "        \n",
    "        for bar, acc in zip(bars1, model_accuracies):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Графік ensemble методів\n",
    "        method_names = [name.replace('ensemble_', '').replace('_', ' ').title() \n",
    "                       for name in ensemble_methods.keys()]\n",
    "        method_accuracies = [history['best_val_acc'] for history in ensemble_methods.values()]\n",
    "        \n",
    "        bars2 = ax2.bar(method_names, method_accuracies, color='lightgreen', alpha=0.7)\n",
    "        ax2.set_title('Ensemble Methods Performance')\n",
    "        ax2.set_ylabel('Validation Accuracy (%)')\n",
    "        ax2.set_xticklabels(method_names, rotation=45)\n",
    "        \n",
    "        for bar, acc in zip(bars2, method_accuracies):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.results_dir}/ensemble_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_overall_comparison(self):\n",
    "        \"\"\"Загальний графік порівняння всіх експериментів\"\"\"\n",
    "        # Виберемо найкращі результати з кожної категорії\n",
    "        best_results = {}\n",
    "        \n",
    "        # Найкраща аугментація\n",
    "        aug_history = {k: v for k, v in self.training_history.items() if k.startswith('aug_')}\n",
    "        if aug_history:\n",
    "            best_aug = max(aug_history.items(), key=lambda x: x[1]['best_val_acc'])\n",
    "            best_results['Best Augmentation'] = best_aug[1]['best_val_acc']\n",
    "        \n",
    "        # Найкраща регуляризація\n",
    "        reg_history = {k: v for k, v in self.training_history.items() if k.startswith('reg_')}\n",
    "        if reg_history:\n",
    "            best_reg = max(reg_history.items(), key=lambda x: x[1]['best_val_acc'])\n",
    "            best_results['Best Regularization'] = best_reg[1]['best_val_acc']\n",
    "        \n",
    "        # Найкращий Ensemble\n",
    "        ensemble_history = {k: v for k, v in self.training_history.items() if 'ensemble_' in k and any(m in k for m in ['average', 'weighted', 'voting'])}\n",
    "        if ensemble_history:\n",
    "            best_ensemble = max(ensemble_history.items(), key=lambda x: x[1]['best_val_acc'])\n",
    "            best_results['Best Ensemble'] = best_ensemble[1]['best_val_acc']\n",
    "        \n",
    "        if best_results:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            methods = list(best_results.keys())\n",
    "            accuracies = list(best_results.values())\n",
    "            \n",
    "            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "            bars = plt.bar(methods, accuracies, color=colors[:len(methods)])\n",
    "            \n",
    "            plt.title('Overall Best Results Comparison')\n",
    "            plt.ylabel('Validation Accuracy (%)')\n",
    "            plt.ylim(0, max(accuracies) * 1.1 if accuracies else 100)\n",
    "            \n",
    "            for bar, acc in zip(bars, accuracies):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                        f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "            \n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{self.results_dir}/overall_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "    def save_final_report(self):\n",
    "        \"\"\"Зберігає фінальний звіт\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f'{self.results_dir}/final_report_{timestamp}.txt'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(\"Augmentation Study - Final Report\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total models trained: {len(self.training_history)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"TRAINING RESULTS SUMMARY:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            for model_name, history in sorted(self.training_history.items()):\n",
    "                f.write(f\"\\n{model_name}:\\n\")\n",
    "                f.write(f\"  Best Validation Accuracy: {history['best_val_acc']:.2f}%\\n\")\n",
    "                if history['train_accs']:\n",
    "                    f.write(f\"  Final Train Accuracy: {history['train_accs'][-1]:.2f}%\\n\")\n",
    "                if history['val_accs']:\n",
    "                    f.write(f\"  Final Val Accuracy: {history['val_accs'][-1]:.2f}%\\n\")\n",
    "                f.write(f\"  Epochs: {history['epochs']}\\n\")\n",
    "        \n",
    "        print(f\"Final report saved to: {report_filename}\")\n",
    "\n",
    "# Запуск дослідження\n",
    "if __name__ == \"__main__\":\n",
    "    study = AugmentationStudy(data_dir='../data')\n",
    "    \n",
    "    try:\n",
    "        # Запуск експериментів\n",
    "        study.experiment_1_augmentation_comparison()\n",
    "        study.experiment_2_regularization_methods()\n",
    "        study.experiment_3_ensemble_models()\n",
    "        \n",
    "        # Фінальні графіки порівняння та звіт\n",
    "        study.plot_comparison_results()\n",
    "        study.save_final_report()\n",
    "        \n",
    "        print(\"\\nAll experiments completed successfully!\")\n",
    "        print(f\"Check the '{study.results_dir}' folder for results.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiments: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
