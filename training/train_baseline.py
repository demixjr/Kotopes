import os
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  
sys.path.insert(0, project_root)

from models.custom_cnn import CustomCNN
from utils.data_loader import get_data_loaders
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  

DATA_DIR = os.path.join(project_root, "data")
CHECKPOINTS_DIR = os.path.join(project_root, "checkpoints") 
RESULTS_DIR = os.path.join(project_root, "results")

class BaselineTrainer:
    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        
        # –Ü—Å—Ç–æ—Ä—ñ—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.best_val_accuracy = 0
        
    def train_epoch(self):
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc="Training")
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # Forward pass
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        epoch_loss = running_loss / len(self.train_loader)
        epoch_accuracy = 100. * correct / total
        
        return epoch_loss, epoch_accuracy
    
    def validate_epoch(self):
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in tqdm(self.val_loader, desc="Validation"):
                data, targets = data.to(self.device), targets.to(self.device)
                
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        epoch_loss = running_loss / len(self.val_loader)
        epoch_accuracy = 100. * correct / total
        
        return epoch_loss, epoch_accuracy
    
    def train(self, epochs, early_stopping_patience=5):
        print(f"–ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö.")
        print(f"–†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞ –ø—ñ—Å–ª—è {early_stopping_patience} –µ–ø–æ—Ö –±–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        
        patience_counter = 0
        
        for epoch in range(epochs):
            print(f"\n{'='*60}")
            print(f"–ï–ø–æ—Ö–∞ {epoch+1}/{epochs}")
            print(f"{'='*60}")
            
            # –ù–∞–≤—á–∞–Ω–Ω—è
            train_loss, train_acc = self.train_epoch()
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            val_loss, val_acc = self.validate_epoch()
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            print(f"\n –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –µ–ø–æ—Ö–∏ {epoch+1}:")
            print(f"   –ù–∞–≤—á–∞–Ω–Ω—è - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%")
            print(f"   –í–∞–ª—ñ–¥–∞—Ü—ñ—è - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
            if val_acc > self.best_val_accuracy:
                self.best_val_accuracy = val_acc
                patience_counter = 0
                self.save_checkpoint(epoch)
                print(f"–ù–æ–≤–∞ –Ω–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å. Accuracy: {val_acc:.2f}%")
            else:
                patience_counter += 1
                print(f"–ë–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {patience_counter}/{early_stopping_patience}")
            
            # –†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞
            if patience_counter >= early_stopping_patience:
                print(f"–†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞ –Ω–∞ –µ–ø–æ—Å—ñ {epoch+1}")
                break
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
        self.load_best_checkpoint()
        return self.best_val_accuracy
    
    def save_checkpoint(self, epoch):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_accuracy': self.best_val_accuracy,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
        }
        
        os.makedirs(CHECKPOINTS_DIR, exist_ok=True)
        torch.save(checkpoint, os.path.join(CHECKPOINTS_DIR, 'baseline_best.pth'))
    
    def load_best_checkpoint(self):
        checkpoint_path = os.path.join(CHECKPOINTS_DIR, 'baseline_best.pth')
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("–ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞!")
    
    def plot_learning_curves(self):
        os.makedirs(os.path.join(RESULTS_DIR, 'baseline'), exist_ok=True)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # –ì—Ä–∞—Ñ—ñ–∫ –≤—Ç—Ä–∞—Ç
        ax1.plot(self.train_losses, label='Train Loss', linewidth=2)
        ax1.plot(self.val_losses, label='Val Loss', linewidth=2)
        ax1.set_title('Learning Curves - Loss', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ—ñ–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ
        ax2.plot(self.train_accuracies, label='Train Accuracy', linewidth=2)
        ax2.plot(self.val_accuracies, label='Val Accuracy', linewidth=2)
        ax2.set_title('Learning Curves - Accuracy', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # –¶—ñ–ª—å–æ–≤–∞ –ª—ñ–Ω—ñ—è 60%
        ax2.axhline(y=60, color='red', linestyle='--', alpha=0.7, label='Target 60%')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'baseline', 'learning_curves.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_results(self):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ç–µ–∫—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª"""
        results_dir = os.path.join(RESULTS_DIR, 'baseline')
        os.makedirs(results_dir, exist_ok=True)
        
        with open(os.path.join(results_dir, 'training_results.txt'), 'w') as f:
            f.write("=== –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ù–ê–í–ß–ê–ù–ù–Ø BASELINE CNN ===\n\n")
            f.write(f"–ù–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: {self.best_val_accuracy:.2f}%\n")
            f.write(f"–§—ñ–Ω–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –Ω–∞–≤—á–∞–Ω–Ω—ñ: {self.train_accuracies[-1]:.2f}%\n")
            f.write(f"–§—ñ–Ω–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: {self.val_accuracies[-1]:.2f}%\n")
            f.write(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö: {len(self.train_accuracies)}\n\n")
            
            f.write("–Ü—Å—Ç–æ—Ä—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è:\n")
            f.write("Epoch | Train Loss | Val Loss | Train Acc | Val Acc\n")
            f.write("-" * 50 + "\n")
            for i in range(len(self.train_losses)):
                f.write(f"{i+1:3d}   | {self.train_losses[i]:.4f}    | {self.val_losses[i]:.4f}  | "
                       f"{self.train_accuracies[i]:6.2f}%  | {self.val_accuracies[i]:6.2f}%\n")

def main():
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
    CONFIG = {
        'batch_size': 32,
        'epochs': 20,
        'learning_rate': 0.001,
        'input_size': 128,
        'early_stopping_patience': 5
    }
    
    # –ü—Ä–∏—Å—Ç—Ä—ñ–π
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
    print("üìÅ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")
    train_loader, val_loader, class_names = get_data_loaders(
        DATA_DIR, 
        input_size=CONFIG['input_size'], 
        batch_size=CONFIG['batch_size']
    )
    
    # –ú–æ–¥–µ–ª—å
    print("üîÑ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ...")
    model = CustomCNN(
        num_classes=len(class_names), 
        input_size=CONFIG['input_size']
    ).to(device)
    
    # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
    
    # –¢—Ä–µ–Ω–µ—Ä
    trainer = BaselineTrainer(model, train_loader, val_loader, criterion, optimizer, device)
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    best_accuracy = trainer.train(
        epochs=CONFIG['epochs'],
        early_stopping_patience=CONFIG['early_stopping_patience']
    )
    
    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    trainer.plot_learning_curves()
    trainer.save_results()
    
    # –§—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\n{'='*50}")
    print("üéØ –§–Ü–ù–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    print(f"{'='*50}")
    print(f"–ù–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: {best_accuracy:.2f}%")
    
    if best_accuracy >= 60:
        print("–¶–Ü–õ–¨ –î–û–°–Ø–ì–ù–£–¢–ê! Accuracy ‚â• 60%")
    else:
        print(" –¶—ñ–ª—å –Ω–µ –¥–æ—Å—è–≥–Ω—É—Ç–∞. –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –º–æ–¥–µ–ª—å.")
    print(f"{'='*50}")

if __name__ == "__main__":
    main()